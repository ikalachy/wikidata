#!/bin/bash

PROJECT=mco-wiki
#PROJECT=bigquery-public-data-staging
BUCKET=mco-wiki
#BUCKET=wikidata
SRC_BASE=https://dumps.wikimedia.org
DST_BASE=gs://$BUCKET
SRC_DATA_PATH=wikidatawiki/entities/latest-all.json.bz
DST_DATA_PATH=wikidata/latest-all.json.bz
SRC_VIEW_PATH=other/pageviews
DST_VIEW_PATH=dumps.wikimedia.org/other/pageviews
FILE_LIST=file-list.tsv

SRC_DATA_URL=$SRC_BASE/$SRC_DATA_PATH
DST_DATA_URL=$DST_BASE/$DST_DATA_PATH
SRC_VIEW_URL=$SRC_BASE/$SRC_VIEW_PATH
DST_VIEW_URL=$DST_BASE/$DST_VIEW_PATH
FILE_LIST_URL=gs://$BUCKET/$FILE_LIST

gcloud config set project $PROJECT

echo "TsvHttpData-1.0" >$FILE_LIST

# if today is Sunday, arrange to get compressed wikidata file.
if [[ $(date +%u) -eq 0 ]]
then
    echo $SRC_DATA_URL >>$FILE_LIST
fi

# Assemble list of every pageview log file and size on website.
wget -nv --spider -S -r -A ".gz" -I $SRC_VIEW_PATH $SRC_VIEW_URL 2>&1 |
  awk 'function base(file, a, n) {n = split(file,a,"/"); return a[n]} \
       $1 == "Content-Length:" {len=$2} $3 == "URL:" {print base($4), len}' |
  sort >src-files.txt

# Assemble list of every pageview log file and size in cloud storage.
gsutil ls -l $DST_VIEW_URL/*/* |
  awk 'function base(file, a, n) {n = split(file,a,"/"); return a[n]} \
       $1 != "TOTAL:" {print base($3), $1}' | sort >dst-files.txt

# One-sided diff - every file that doesn't exist or match size in cloud storage.
comm -23 src-files.txt dst-files.txt |
while read FILE SIZE
do
  echo $SRC_VIEW_URL/$FILE >>$FILE_LIST
done

gsutil cp $FILE_LIST $FILE_LIST_URL
